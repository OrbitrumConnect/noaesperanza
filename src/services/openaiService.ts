// Servi√ßo para comunica√ß√£o com OpenAI API
import { getNoaSystemPrompt } from '../config/noaSystemPrompt'
import { personalizedProfilesService } from './personalizedProfilesService'
import { loadNoaPrompt, logPromptInitialization } from './noaPromptLoader'

export interface ChatMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

export interface OpenAIResponse {
  choices: Array<{
    message: {
      content: string
      role: string
    }
  }>
}

class OpenAIService {
  private apiKey: string
  private baseURL = 'https://api.openai.com/v1'
  private noaAgentId = 'asst_fN2Fk9fQ7JEyyFUIe50Fo9QD' // Agente espec√≠fico da NOA
  private fineTunedModel = 'ft:gpt-3.5-turbo-0125:personal:fine-tuning-noa-esperanza-avaliacao-inicial-dez-ex-jsonl:BR0W02VP' // Modelo fine-tuned da NOA

  constructor() {
    this.apiKey = import.meta.env.VITE_OPENAI_API_KEY
    if (!this.apiKey) {
      console.warn('OpenAI API Key n√£o encontrada - ativando modo offline')
    }
    
    // Log de inicializa√ß√£o do prompt mestre
    logPromptInitialization()
  }

  // M√©todo para usar modelo padr√£o (n√£o fine-tuned)
  async sendMessageWithStandardModel(
    messages: ChatMessage[], 
    systemPrompt?: string
  ): Promise<string> {
    try {
      const requestMessages: ChatMessage[] = []
      
      // Adiciona prompt do sistema se fornecido
      if (systemPrompt) {
        requestMessages.push({
          role: 'system',
          content: systemPrompt
        })
      }
      
      // Adiciona mensagens da conversa
      requestMessages.push(...messages)

      if (!this.apiKey) {
        return this.offlineResponse(messages)
      }

      const response = await fetch(`${this.baseURL}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo', // Usando modelo padr√£o
          messages: requestMessages,
          max_tokens: 1000,
          temperature: 0.7,
          stream: false
        })
      })

      if (!response.ok) {
        const errorData = await response.json()
        if (response.status === 401) {
          console.warn('OpenAI 401 - usando fallback offline')
          return this.offlineResponse(messages)
        }
        throw new Error(`OpenAI API Error: ${errorData.error?.message || 'Erro desconhecido'}`)
      }

      const data: OpenAIResponse = await response.json()
      
      if (data.choices && data.choices.length > 0) {
        return data.choices[0].message.content
      }
      
      throw new Error('Resposta vazia da OpenAI')
      
    } catch (error) {
      console.error('Erro ao comunicar com OpenAI:', error)
      return this.offlineResponse(messages)
    }
  }

  async sendMessage(
    messages: ChatMessage[], 
    systemPrompt?: string
  ): Promise<string> {
    try {
      const requestMessages: ChatMessage[] = []
      
      // Adiciona prompt do sistema se fornecido
      if (systemPrompt) {
        requestMessages.push({
          role: 'system',
          content: systemPrompt
        })
      }
      
      // Adiciona mensagens da conversa
      requestMessages.push(...messages)

      if (!this.apiKey) {
        return this.offlineResponse(messages)
      }

      const response = await fetch(`${this.baseURL}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo', // Usando modelo padr√£o em vez do fine-tuned
          messages: requestMessages,
          max_tokens: 1000,
          temperature: 0.7,
          stream: false
        })
      })

      if (!response.ok) {
        const errorData = await response.json()
        if (response.status === 401) {
          console.warn('OpenAI 401 - usando fallback offline')
          return this.offlineResponse(messages)
        }
        throw new Error(`OpenAI API Error: ${errorData.error?.message || 'Erro desconhecido'}`)
      }

      const data: OpenAIResponse = await response.json()
      
      if (data.choices && data.choices.length > 0) {
        return data.choices[0].message.content
      }
      
      throw new Error('Resposta vazia da OpenAI')
      
    } catch (error) {
      console.error('Erro ao comunicar com OpenAI:', error)
      return this.offlineResponse(messages)
    }
  }

  // M√©todo para usar o Assistants API com agente espec√≠fico
  async useNoaAgent(userMessage: string, threadId?: string): Promise<{ response: string, threadId: string }> {
    try {
      console.log('ü§ñ Usando agente NOA espec√≠fico:', this.noaAgentId)
      
      let currentThreadId = threadId
      
      // Criar thread se n√£o existir
      if (!currentThreadId) {
        const threadResponse = await fetch(`${this.baseURL}/threads`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${this.apiKey}`,
            'OpenAI-Beta': 'assistants=v2'
          },
          body: JSON.stringify({})
        })
        
        if (!threadResponse.ok) {
          throw new Error('Erro ao criar thread')
        }
        
        const threadData = await threadResponse.json()
        currentThreadId = threadData.id
        console.log('üßµ Thread criada:', currentThreadId)
      }
      
      // Adicionar mensagem do usu√°rio
      await fetch(`${this.baseURL}/threads/${currentThreadId}/messages`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`,
          'OpenAI-Beta': 'assistants=v2'
        },
        body: JSON.stringify({
          role: 'user',
          content: userMessage
        })
      })
      
      // Executar agente
      const runResponse = await fetch(`${this.baseURL}/threads/${currentThreadId}/runs`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`,
          'OpenAI-Beta': 'assistants=v2'
        },
        body: JSON.stringify({
          assistant_id: this.noaAgentId
        })
      })
      
      if (!runResponse.ok) {
        throw new Error('Erro ao executar agente')
      }
      
      const runData = await runResponse.json()
      console.log('üèÉ Run iniciado:', runData.id)
      
      // Aguardar conclus√£o
      let runStatus = 'in_progress'
      while (runStatus === 'in_progress' || runStatus === 'queued') {
        await new Promise(resolve => setTimeout(resolve, 1000))
        
        const statusResponse = await fetch(`${this.baseURL}/threads/${currentThreadId}/runs/${runData.id}`, {
          headers: {
            'Authorization': `Bearer ${this.apiKey}`,
            'OpenAI-Beta': 'assistants=v2'
          }
        })
        
        const statusData = await statusResponse.json()
        runStatus = statusData.status
        console.log('üìä Status do run:', runStatus)
      }
      
      if (runStatus !== 'completed') {
        throw new Error(`Run falhou com status: ${runStatus}`)
      }
      
      // Obter mensagens da thread
      const messagesResponse = await fetch(`${this.baseURL}/threads/${currentThreadId}/messages`, {
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'OpenAI-Beta': 'assistants=v2'
        }
      })
      
      const messagesData = await messagesResponse.json()
      const lastMessage = messagesData.data[0] // √öltima mensagem √© a resposta do agente
      
      return {
        response: lastMessage.content[0].text.value,
        threadId: currentThreadId || ''
      }
      
    } catch (error) {
      console.error('Erro ao usar agente NOA:', error)
      // Fallback para m√©todo tradicional
      return {
        response: await this.getNoaResponseFallback(userMessage),
        threadId: ''
      }
    }
  }

  // M√©todo espec√≠fico para NOA - Assistente M√©dica com base de conhecimento pr√≥pria
  async getNoaResponse(userMessage: string, conversationHistory: ChatMessage[] = [], knowledgeBase?: string): Promise<string> {
    try {
      console.log('üéØ Usando modelo padr√£o com base de conhecimento pr√≥pria')
      
      // Verificar se h√° perfil ativo
      const activeProfile = personalizedProfilesService.getActiveProfile()
      
      // Construir prompt com sistema completo da N√¥a Esperanza V2.0
      let systemPrompt = ''
      
      if (activeProfile) {
        // Usar prompt personalizado do perfil
        systemPrompt = `${getNoaSystemPrompt({
          name: activeProfile.name,
          role: activeProfile.role,
          recognizedAs: activeProfile.name
        })}\n\n${activeProfile.systemPrompt}`
      } else {
        // Verificar formato antigo para compatibilidade
        try {
          const stored = localStorage.getItem('noa_recognized_user')
          if (stored) {
            const recognizedUser = JSON.parse(stored)
            systemPrompt = getNoaSystemPrompt({
              name: recognizedUser.name,
              role: recognizedUser.role,
              recognizedAs: recognizedUser.name
            })
          } else {
            systemPrompt = getNoaSystemPrompt({
              name: 'Usu√°rio',
              role: 'user'
            })
          }
        } catch (e) {
          console.warn('Erro ao carregar usu√°rio reconhecido:', e)
          systemPrompt = getNoaSystemPrompt({
            name: 'Usu√°rio',
            role: 'user'
          })
        }
      }

      // Adicionar base de conhecimento se fornecida
      if (knowledgeBase) {
        systemPrompt += `\n\nBASE DE CONHECIMENTO DA N√îA ESPERANZA:\n${knowledgeBase}\n\nIMPORTANTE: Use SEMPRE as informa√ß√µes da base de conhecimento acima. N√ÉO invente informa√ß√µes sobre "Weme" ou outras empresas.`
      }

      const messages: ChatMessage[] = [
        ...conversationHistory,
        {
          role: 'user',
          content: userMessage
        }
      ]

      // Usar modelo padr√£o em vez do fine-tuned externo
      return this.sendMessageWithStandardModel(messages, systemPrompt)
      
    } catch (error) {
      console.log('üîÑ Erro no modelo padr√£o, usando fallback')
      return this.getNoaResponseFallback(userMessage, conversationHistory)
    }
  }

  // M√©todo fallback tradicional
  private async getNoaResponseFallback(userMessage: string, conversationHistory: ChatMessage[] = []): Promise<string> {
    // Verificar se h√° perfil ativo
    const activeProfile = personalizedProfilesService.getActiveProfile()
    
    // Usar prompt do sistema V2.0
    let systemPrompt = ''
    
    if (activeProfile) {
      systemPrompt = `${getNoaSystemPrompt({
        name: activeProfile.name,
        role: activeProfile.role,
        recognizedAs: activeProfile.name
      })}\n\n${activeProfile.systemPrompt}`
    } else {
      try {
        const stored = localStorage.getItem('noa_recognized_user')
        if (stored) {
          const recognizedUser = JSON.parse(stored)
          systemPrompt = getNoaSystemPrompt({
            name: recognizedUser.name,
            role: recognizedUser.role,
            recognizedAs: recognizedUser.name
          })
        } else {
          systemPrompt = getNoaSystemPrompt({
            name: 'Usu√°rio',
            role: 'user'
          })
        }
      } catch (e) {
        console.warn('Erro ao carregar usu√°rio reconhecido:', e)
        systemPrompt = getNoaSystemPrompt({
          name: 'Usu√°rio',
          role: 'user'
        })
      }
    }

    const messages: ChatMessage[] = [
      ...conversationHistory,
      {
        role: 'user',
        content: userMessage
      }
    ]

    return this.sendMessage(messages, systemPrompt)
  }

  // Fallback inteligente offline
  private offlineResponse(messages: ChatMessage[]): string {
    const lastUser = [...messages].reverse().find(m => m.role === 'user')
    const userText = (lastUser?.content || '').toLowerCase().trim()
    
    // Sauda√ß√µes
    if (userText.match(/^(oi|ol√°|ola|hey|bom dia|boa tarde|boa noite|e ai)/)) {
      return `Ol√°! Sou a N√¥a Esperanza üåø\n\nMuito prazer em conhec√™-lo! Estou aqui para ajudar com sua sa√∫de e bem-estar.\n\nPosso realizar avalia√ß√£o cl√≠nica completa, orientar sobre cannabis medicinal e muito mais.\n\nComo posso ajudar voc√™ hoje?`
    }
    
    // Como est√° / tudo bem
    if (userText.match(/(tudo bem|como (voc√™|vc|voce) est√°|como vai|legal|beleza)/)) {
      return `Estou √≥tima, muito obrigada por perguntar! üòä\n\nEstou aqui pronta para ajudar com o que voc√™ precisar. Seja para conversar sobre sua sa√∫de, fazer uma avalia√ß√£o cl√≠nica ou tirar d√∫vidas sobre tratamentos.\n\nE voc√™, como est√° se sentindo?`
    }
    
    // Quem √© / sobre voc√™
    if (userText.match(/(quem (√©|e) voc√™|quem (√©|e) noa|quem te criou|sobre voc√™|se apresente)/)) {
      return `Sou a N√¥a Esperanza, assistente m√©dica inteligente especializada em neurologia, cannabis medicinal e nefrologia. üß†üåø\n\nFui desenvolvida pelo Dr. Ricardo e sua equipe para oferecer atendimento humanizado e baseado em evid√™ncias cient√≠ficas.\n\nTenho conhecimento profundo em avalia√ß√£o cl√≠nica pelo protocolo IMRE e posso auxiliar em todo o processo terap√™utico.\n\nEst√° precisando de ajuda com algo espec√≠fico?`
    }
    
    // Avalia√ß√£o cl√≠nica
    if (userText.match(/(avalia|clinica|imre|consulta|sintoma|dor|problema)/)) {
      return `Entendo que voc√™ est√° buscando avalia√ß√£o cl√≠nica. Vou ajudar!\n\nRealizo uma avalia√ß√£o completa seguindo o protocolo IMRE (Identifica√ß√£o, Mol√©stia atual, Revis√£o de sistemas, Exame f√≠sico).\n\nPara come√ßarmos, me conte: qual √© a sua principal queixa ou preocupa√ß√£o de sa√∫de no momento?`
    }
    
    // Cannabis
    if (userText.match(/(cannabis|cbd|thc|oleo|canabidiol)/)) {
      return `Sobre cannabis medicinal, posso te orientar! üåø\n\nA cannabis tem se mostrado eficaz em diversas condi√ß√µes, especialmente dor cr√¥nica, ansiedade, ins√¥nia e condi√ß√µes neurol√≥gicas.\n\nPara prescri√ß√£o adequada, √© importante fazer uma avalia√ß√£o cl√≠nica completa. Assim consigo indicar o melhor produto, dosagem e forma de uso para o seu caso espec√≠fico.\n\nGostaria de come√ßar a avalia√ß√£o agora?`
    }
    
    // Tratamento / medicamento
    if (userText.match(/(tratamento|medicamento|remedio|prescri)/)) {
      return `Sobre tratamentos, trabalho de forma personalizada! üíä\n\nCada pessoa √© √∫nica e merece um plano terap√™utico individual. Posso orientar sobre medicamentos convencionais e cannabis medicinal.\n\nPara criar um plano adequado para voc√™, preciso conhecer melhor sua condi√ß√£o. Quer fazer uma avalia√ß√£o cl√≠nica comigo?`
    }
    
    // Ajuda / n√£o entendi
    if (userText.match(/(ajuda|help|socorro|perdid|n√£o entend)/)) {
      return `Estou aqui para ajudar! ü§ó\n\n**Principais funcionalidades:**\n‚Ä¢ Avalia√ß√£o cl√≠nica completa (IMRE)\n‚Ä¢ Orienta√ß√£o sobre cannabis medicinal\n‚Ä¢ Informa√ß√µes sobre tratamentos\n‚Ä¢ Acompanhamento terap√™utico\n\n√â s√≥ conversar comigo naturalmente! Pode fazer perguntas, contar seus sintomas ou pedir orienta√ß√µes.\n\nSobre o que gostaria de conversar?`
    }
    
    // Fallback mais inteligente e natural
    const responses = [
      `Entendo sua pergunta. No momento estou em modo offline, ent√£o minhas respostas s√£o baseadas no conhecimento que j√° tenho armazenado.\n\nPosso te ajudar especialmente com avalia√ß√£o cl√≠nica e cannabis medicinal. Sobre o que voc√™ perguntou, prefere que eu te oriente sobre esses temas ou quer conversar sobre outra coisa?`,
      
      `Interessante quest√£o! Embora esteja offline agora, tenho conhecimento extenso em sa√∫de e medicina.\n\nSe sua d√∫vida √© sobre sa√∫de, sintomas ou tratamentos, posso te ajudar bastante! Quer me contar mais detalhes?`,
      
      `Vi sua mensagem! üåø\n\nEstou em modo offline, mas ainda assim posso conversar e ajudar com v√°rias coisas, especialmente relacionadas √† sa√∫de.\n\nPoderia reformular sua pergunta ou me contar o que voc√™ est√° precisando? Assim consigo te ajudar melhor!`
    ]
    
    // Escolher resposta aleat√≥ria para parecer mais natural
    return responses[Math.floor(Math.random() * responses.length)]
  }
}

export const openAIService = new OpenAIService()

// üåÄ GPT BUILDER V2 - ENVIAR PARA OPENAI
// Envia prompt completo para GPT-4o com configura√ß√µes otimizadas
export async function sendToOpenAI(fullPrompt: string) {
  const apiKey = import.meta.env.VITE_OPENAI_API_KEY
  
  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: "gpt-4o",
      messages: [
        { role: "system", content: "Voc√™ √© N√¥a Esperanza, assistente cl√≠nica da plataforma, com base simb√≥lica e escuta ativa." },
        { role: "user", content: fullPrompt }
      ],
      temperature: 0.7
    })
  });

  const data = await response.json();
  return data.choices[0].message.content;
}
